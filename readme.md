# Automated Pneumonia Detection Using Deep Learning
## CSCA 5642 Introduction to Deep Learning - Final Project
This repository contains my final project for CSCA 5642 Introduction to Deep Learning, where I developed and compared three different deep learning architectures for automated pneumonia detection from chest X-ray images. The project addresses a critical healthcare challenge where pneumonia remains the leading infectious cause of child mortality globally, claiming over 800,000 children under five years old annually according to the World Health Organization. The shortage of radiologists and time-consuming manual X-ray interpretation in many healthcare facilities, particularly in rural and underserved regions, motivated me to explore whether deep learning could assist in screening and prioritizing urgent pneumonia cases for expert review.

## Project Overview
The goal of this project is to develop an AI-assisted diagnostic tool that can automatically classify chest X-ray images as either showing signs of pneumonia or being normal. I implemented and compared three different model architectures: a custom convolutional neural network built from scratch, and two transfer learning approaches using pre-trained ResNet18 and VGG16 models. By evaluating multiple approaches, I aimed to understand the trade-offs between training from scratch versus leveraging pre-trained ImageNet features for medical imaging applications. The project emphasizes high sensitivity in detecting pneumonia cases, which is crucial in medical screening contexts where missing a true positive diagnosis could delay life-saving treatment.

## Dataset
I used the Chest X-Ray Images Pneumonia dataset from Kaggle, originally collected from Guangzhou Women and Children's Medical Center in China. The dataset contains 5,856 pediatric chest X-ray images organized into training, validation, and test splits. The training set includes 5,216 images with 1,341 normal cases and 3,875 pneumonia cases. The validation set is notably small with only 16 images, which presented challenges during model tuning that I discuss in the results section. The test set contains 624 images with 234 normal and 390 pneumonia cases. All images were reviewed and labeled by expert physicians to ensure accurate ground truth labels. The significant class imbalance with pneumonia cases representing approximately 73 percent of the dataset required careful handling through weighted loss functions during training.

## Technical Approach
I began with exploratory data analysis to understand the characteristics of the chest X-ray images. The analysis revealed substantial variability in image dimensions ranging from 384 by 127 pixels to 2916 by 2713 pixels, as well as differences in intensity patterns between normal and pneumonia cases. Normal X-rays tend to have larger and more variable dimensions with more uniform darkness representing clear lungs, while pneumonia X-rays show increased opacity and infiltrates with higher intensity standard deviation. Based on these findings, I standardized all images to 224 by 224 pixels to match the input requirements of pre-trained models and ensure consistent neural network inputs.

For data preprocessing, I implemented separate transformation pipelines for training versus validation and test sets. The training pipeline includes data augmentation techniques such as random horizontal flips with 50 percent probability, random rotations up to 15 degrees, color jitter for brightness and contrast adjustments, and small random translations. I kept these transformations relatively conservative because medical images are more sensitive to distortions than natural images. The validation and test sets receive only resizing and normalization without augmentation to ensure fair evaluation on unmodified data. All images were normalized using ImageNet statistics with mean values of 0.485, 0.456, and 0.406 and standard deviations of 0.229, 0.224, and 0.225 to match the pre-training of the transfer learning models.

## Model Architectures
I implemented three distinct model architectures to compare different deep learning approaches for pneumonia detection. The first model is a custom convolutional neural network built from scratch with four convolutional blocks that progressively increase feature channels from 32 to 64 to 128 to 256 filters while reducing spatial dimensions through max pooling. Each block includes batch normalization for training stability, ReLU activation for non-linearity, and 25 percent dropout to prevent overfitting. The feature extractor feeds into a fully connected classifier with 512 hidden units and 50 percent dropout before the final binary classification layer. This architecture has approximately 26 million trainable parameters and serves as my baseline to understand what can be achieved training from scratch on this medical imaging dataset.

The second model uses ResNet18 with transfer learning, leveraging residual connections that enable deeper networks without vanishing gradients. I loaded ImageNet pre-trained weights and froze the early layers, keeping only the last 10 parameters trainable to preserve general low-level feature detectors while allowing task-specific fine-tuning. The final fully connected layer was replaced to output two classes for binary pneumonia classification. This approach significantly reduces the number of trainable parameters while benefiting from features learned on millions of natural images.

The third model employs VGG16 with transfer learning, using a deeper architecture with 13 convolutional layers arranged in blocks with increasing depth. Similar to ResNet18, I froze early layers and fine-tuned only the final layers, replacing the classifier head for binary classification. While VGG16 has more total parameters than ResNet18, the transfer learning approach keeps most frozen, making training feasible on my hardware with limited GPU memory. I chose a batch size of 16 due to my M1 Mac's GPU memory constraints after initially trying batch size 32 which caused memory errors.

## Training Methodology
To address the dataset’s strong class imbalance (2.7:1 pneumonia-to-normal), I used weighted cross-entropy loss, assigning higher weight to the minority Normal class. This prevented the model from defaulting to predicting pneumonia for most images—a behavior I observed in early experiments without class weighting.

The training process followed a standard setup with separate training and validation loops. Each epoch involved computing weighted loss, updating parameters with backpropagation, and evaluating the model on the validation set without gradients. I tracked training/validation loss and accuracy across epochs to monitor learning progress and watch for overfitting.

To improve training stability, I added a ReduceLROnPlateau scheduler that cuts the learning rate by 50% when validation loss stops improving, and I used early stopping with a patience of 5 epochs. This prevented overfitting and avoided stopping too early, which happened when patience was set to just 1 epoch. The best-performing model on the validation set was saved automatically for final testing.


## Results
All three models were evaluated on the test set of 624 images using a comprehensive suite of metrics, including accuracy, precision, recall, F1-score, sensitivity, specificity, and ROC AUC. The results highlight clear performance differences and trade-offs, particularly between sensitivity and specificity.

CustomCNN achieved 87.50% accuracy, with 90.51% sensitivity and 82.48% specificity. Its ROC AUC of 0.9443 reflects strong overall discriminative capability. While it doesn’t lead in any single metric, the custom model demonstrates that a lightweight architecture trained from scratch can still deliver solid pneumonia-detection performance without relying on transfer learning.

ResNet18 delivered the best overall performance, reaching 92.15% accuracy and the highest ROC AUC of 0.9685. It also achieved 94.62% sensitivity and 88.03% specificity, offering a strong balance between catching pneumonia cases and avoiding false positives. This high sensitivity is particularly important in clinical settings, where missing a pneumonia case could delay critical treatment.

VGG16 produced a competitive overall accuracy of 90.87% and an ROC AUC of 0.9588. It achieved 94.10% sensitivity, nearly matching ResNet18, but at the cost of lower specificity (85.47%). This means VGG16 tends to generate more false positives, favoring caution by flagging borderline cases as pneumonia. Although effective at detecting pneumonia, the model’s training curves showed early convergence and signs of overfitting, with validation loss rising while training loss continued to decline.

Overall, ResNet18 stands out as the most reliable and well-balanced model, offering the strongest combination of sensitivity, specificity, and discriminative power for pneumonia classification.


## Key Learnings
This project taught me several important lessons about applying deep learning to medical imaging. Transfer learning proved especially effective for small datasets—both ResNet18 and VGG16 outperformed my custom CNN, confirming that ImageNet-trained features still transfer well to chest X-ray images. Even so, the CustomCNN’s solid performance showed that simple architectures can still work if designed carefully.

Handling class imbalance was another key learning. Using weighted loss was essential; without it, the models tended to predict pneumonia for most images. I also developed a deeper appreciation for the sensitivity–specificity trade-off. In medical screening, high sensitivity is often more critical, which supports the practicality of using models like ResNet18 that balance strong accuracy with high sensitivity.

The project also highlighted the limitations of very small validation sets. With only 16 validation images, training curves were extremely noisy and unreliable. This reinforced the importance of having enough validation data to make informed decisions about hyperparameters and model selection.

## Challenges and Future Improvements
The biggest challenge was the tiny validation set, which made model evaluation unstable. A more balanced data split—such as 70:20:10, would have provided more reliable validation results. My hyperparameter tuning was also limited, relying mostly on defaults rather than structured search methods, which likely restricted model performance.

Another limitation was freezing too many layers in the transfer learning models. Allowing more layers to fine-tune could improve adaptation to the medical imaging domain.

For future work, I would explore model ensembling, add interpretability techniques such as Grad-CAM, and test more modern architectures like EfficientNet or Vision Transformers. Most importantly, expanding the dataset—either by adding more local images or incorporating external medical imaging datasets—would improve model generalization and reduce bias, ultimately making the system more reliable for real-world clinical use.