# Automated Pneumonia Detection Using Deep Learning
## CSCA 5642 Introduction to Deep Learning - Final Project
This repository contains my final project for CSCA 5642 Introduction to Deep Learning, where I developed and compared three different deep learning architectures for automated pneumonia detection from chest X-ray images. The project addresses a critical healthcare challenge where pneumonia remains the leading infectious cause of child mortality globally, claiming over 800,000 children under five years old annually according to the World Health Organization. The shortage of radiologists and time-consuming manual X-ray interpretation in many healthcare facilities, particularly in rural and underserved regions, motivated me to explore whether deep learning could assist in screening and prioritizing urgent pneumonia cases for expert review.

## Project Overview
The goal of this project is to develop an AI-assisted diagnostic tool that can automatically classify chest X-ray images as either showing signs of pneumonia or being normal. I implemented and compared three different model architectures: a custom convolutional neural network built from scratch, and two transfer learning approaches using pre-trained ResNet18 and VGG16 models. By evaluating multiple approaches, I aimed to understand the trade-offs between training from scratch versus leveraging pre-trained ImageNet features for medical imaging applications. The project emphasizes high sensitivity in detecting pneumonia cases, which is crucial in medical screening contexts where missing a true positive diagnosis could delay life-saving treatment.

## Dataset
I used the Chest X-Ray Images Pneumonia dataset from Kaggle, originally collected from Guangzhou Women and Children's Medical Center in China. The dataset contains 5,856 pediatric chest X-ray images organized into training, validation, and test splits. The training set includes 5,216 images with 1,341 normal cases and 3,875 pneumonia cases. The validation set is notably small with only 16 images, which presented challenges during model tuning that I discuss in the results section. The test set contains 624 images with 234 normal and 390 pneumonia cases. All images were reviewed and labeled by expert physicians to ensure accurate ground truth labels. The significant class imbalance with pneumonia cases representing approximately 73 percent of the dataset required careful handling through weighted loss functions during training.

## Technical Approach
I began with exploratory data analysis to understand the characteristics of the chest X-ray images. The analysis revealed substantial variability in image dimensions ranging from 384 by 127 pixels to 2916 by 2713 pixels, as well as differences in intensity patterns between normal and pneumonia cases. Normal X-rays tend to have larger and more variable dimensions with more uniform darkness representing clear lungs, while pneumonia X-rays show increased opacity and infiltrates with higher intensity standard deviation. Based on these findings, I standardized all images to 224 by 224 pixels to match the input requirements of pre-trained models and ensure consistent neural network inputs.

For data preprocessing, I implemented separate transformation pipelines for training versus validation and test sets. The training pipeline includes data augmentation techniques such as random horizontal flips with 50 percent probability, random rotations up to 15 degrees, color jitter for brightness and contrast adjustments, and small random translations. I kept these transformations relatively conservative because medical images are more sensitive to distortions than natural images. The validation and test sets receive only resizing and normalization without augmentation to ensure fair evaluation on unmodified data. All images were normalized using ImageNet statistics with mean values of 0.485, 0.456, and 0.406 and standard deviations of 0.229, 0.224, and 0.225 to match the pre-training of the transfer learning models.

## ## Model Architectures
I implemented three distinct model architectures to compare different deep learning approaches for pneumonia detection. The first model is a custom convolutional neural network built from scratch with four convolutional blocks that progressively increase feature channels from 32 to 64 to 128 to 256 filters while reducing spatial dimensions through max pooling. Each block includes batch normalization for training stability, ReLU activation for non-linearity, and 25 percent dropout to prevent overfitting. The feature extractor feeds into a fully connected classifier with 512 hidden units and 50 percent dropout before the final binary classification layer. This architecture has approximately 26 million trainable parameters and serves as my baseline to understand what can be achieved training from scratch on this medical imaging dataset.

The second model uses ResNet18 with transfer learning, leveraging residual connections that enable deeper networks without vanishing gradients. I loaded ImageNet pre-trained weights and froze the early layers, keeping only the last 10 parameters trainable to preserve general low-level feature detectors while allowing task-specific fine-tuning. The final fully connected layer was replaced to output two classes for binary pneumonia classification. This approach significantly reduces the number of trainable parameters while benefiting from features learned on millions of natural images.

The third model employs VGG16 with transfer learning, using a deeper architecture with 13 convolutional layers arranged in blocks with increasing depth. Similar to ResNet18, I froze early layers and fine-tuned only the final layers, replacing the classifier head for binary classification. While VGG16 has more total parameters than ResNet18, the transfer learning approach keeps most frozen, making training feasible on my hardware with limited GPU memory. I chose a batch size of 16 due to my M1 Mac's GPU memory constraints after initially trying batch size 32 which caused memory errors.

## Training Methodology
To handle the significant class imbalance in the dataset with a 2.70 to 1 pneumonia-to-normal ratio, I implemented weighted cross-entropy loss by assigning higher weight to the minority Normal class. This prevents the model from developing a bias toward always predicting the majority Pneumonia class, which is critical in medical applications where both false positives and false negatives have consequences. Without this weighting, my early experiments showed models that simply predicted pneumonia for almost everything to minimize overall loss.

The training loop follows a standard approach with separate training and validation functions. During each training epoch, the model processes batches of images, computes predictions, calculates weighted loss, and updates parameters via backpropagation. The validation loop runs without gradient computation to evaluate performance on unseen data. I track four metrics across epochs including training loss, training accuracy, validation loss, and validation accuracy to monitor both model performance and potential overfitting.

To optimize training and prevent overfitting, I implemented several strategies. Learning rate scheduling with ReduceLROnPlateau automatically reduces the learning rate by 50 percent if validation loss plateaus for 2 consecutive epochs, allowing finer-grained optimization as training progresses. Early stopping with patience of 5 epochs halts training if validation accuracy stops improving, preventing unnecessary computation and overfitting to the training set. I initially set patience to 1 epoch but found this too aggressive as models would stop before fully exploring the loss landscape, so I increased it to 5 epochs. The model with the best validation accuracy is automatically saved for later evaluation on the test set.

For the CustomCNN trained from scratch, I used the Adam optimizer with a learning rate of 0.001. For the transfer learning models ResNet18 and VGG16, I used a lower learning rate of 0.0001 since we are fine-tuning pre-trained weights and do not want to destroy the learned features too quickly. This training framework is applied consistently to all three models, enabling fair comparison of their pneumonia detection capabilities.

## Results
After training all three models, I evaluated their performance on the unseen test set of 624 images using comprehensive classification metrics including accuracy, precision, recall, F1-score, sensitivity, specificity, and ROC AUC. The results reveal clear differences in model capabilities and trade-offs between sensitivity and specificity.

CustomCNN achieved 82.53 percent accuracy with 80.77 percent sensitivity and 85.47 percent specificity. The ROC AUC of 0.9193 indicates good overall discriminative capability. While it does not excel in any single metric, the custom CNN demonstrates that a relatively simple architecture trained from scratch can achieve reasonable pneumonia detection without transfer learning. The training curves showed steady convergence over 8 epochs with mild overfitting as training accuracy reached approximately 90 percent while validation accuracy plateaued around 80 percent.

ResNet18 emerged as the best overall performer with 91.83 percent accuracy and the highest ROC AUC score of 0.9654, indicating excellent discriminative ability. Most importantly for pneumonia screening, it achieves 96.41 percent sensitivity, meaning it correctly identifies 96.41 percent of pneumonia cases. This high sensitivity is critical in medical applications where missing a true pneumonia diagnosis could delay life-saving treatment. The specificity of 84.19 percent means some healthy patients may be flagged as potentially having pneumonia, but in a screening context these false positives can be corrected with follow-up examinations. The training history showed fast initial learning with near-perfect training accuracy of 97 percent by the end, though validation metrics fluctuated due to the extremely small validation set of only 16 images.

VGG16 achieved the highest sensitivity at 96.92 percent, catching nearly all pneumonia cases, but has the lowest specificity at 72.22 percent, resulting in more false positives. This means VGG16 is very cautious and tends to flag borderline cases as pneumonia, which could lead to unnecessary follow-up tests but ensures fewer missed diagnoses. The overall accuracy of 87.66 percent and ROC AUC of 0.9544 remain strong. The training curves showed rapid convergence within just 6 epochs before early stopping, but exhibited clear overfitting patterns with validation loss increasing while training loss continued to decrease.

From a clinical perspective, the sensitivity-specificity trade-off is evident across all models. ResNet18 and VGG16 prioritize high sensitivity at the expense of specificity, while CustomCNN maintains better balance between the two metrics. For a pneumonia screening tool, high sensitivity is generally more valuable since false positives can be corrected with follow-up examinations, whereas false negatives could have serious health consequences. Based on these results, ResNet18 offers the best compromise with excellent sensitivity while maintaining reasonable specificity and the highest overall accuracy, making it the recommended model for potential deployment.

## Key Learnings
Through this project, I gained several valuable insights about applying deep learning to medical imaging. First, transfer learning proved incredibly powerful for small medical datasets, with both ResNet18 and VGG16 significantly outperforming my custom CNN trained from scratch. This confirms that ImageNet pre-trained features do transfer meaningfully to chest X-ray images despite the domain difference between natural images and medical imaging. However, I was surprised that my relatively simple CustomCNN still achieved 82.53 percent accuracy, demonstrating that you do not always need complex architectures if you design thoughtfully.

I learned the critical importance of class imbalance handling in medical applications. Implementing weighted loss functions was essential, as my early experiments without weighting showed models that simply predicted pneumonia for almost everything to minimize the loss on the majority class. Understanding the sensitivity-specificity trade-off was also eye-opening. In medical screening contexts, high sensitivity is often more valuable than high specificity, which explains why ResNet18's 96.41 percent sensitivity makes it the most clinically useful model despite not having the highest overall accuracy.

I also experienced firsthand the limitations of small validation sets. The 16-image validation set caused extremely noisy training curves, especially for ResNet18, making it difficult to trust validation metrics for hyperparameter tuning. This taught me that data quality and quantity matter as much as model architecture. Looking at my training curves, I can see that validation loss fluctuates wildly from epoch to epoch, which would not happen with a properly sized validation set of at least a few hundred images.

## Challenges and Future Improvements
Several challenges emerged during this project that point toward future improvements. The most significant issue was the tiny validation set of only 16 images, which was essentially useless for reliable model evaluation. In retrospect, I should have redistributed the data splits to create a larger validation set, perhaps using a 70-20-10 train-validation-test split instead of the provided splits. This would have enabled more reliable hyperparameter tuning and model selection.

My hyperparameter optimization was minimal, mostly using default values or making educated guesses rather than conducting systematic grid search or random search. This likely left performance on the table, and future work should include more rigorous hyperparameter tuning. Additionally, I froze perhaps too many layers in the transfer learning models. Keeping only the last 10 parameters trainable was conservative, and unfreezing more layers might have improved performance by allowing the models to adapt more to the medical imaging domain.

For future improvements, I would explore ensemble methods that combine predictions from all three models to leverage their complementary strengths. Implementing Grad-CAM visualization would provide interpretability crucial for medical applications, allowing clinicians to see what regions of the X-ray the model focuses on when making predictions. Testing more modern architectures like EfficientNet, DenseNet, or Vision Transformers could provide better accuracy-efficiency trade-offs than the older VGG16 architecture.

The most important improvement would be expanding the dataset by incorporating additional chest X-ray sources or using external datasets for validation. Research has shown that domain-specific transfer learning, where models are pre-trained on large medical imaging datasets rather than ImageNet, often provides superior performance for medical applications. Collecting more diverse data from different hospitals and imaging equipment would also improve the model's generalization to real-world clinical settings and help address potential biases in the training data.